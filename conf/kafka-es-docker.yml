inputs:
    - NewKafka:
        topic:
            test-reporter: 1
        codec: json
        consumer_settings:
            bootstrap.servers: kafka:9092
            value.deserializer: org.apache.kafka.common.serialization.StringDeserializer
            key.deserializer: org.apache.kafka.common.serialization.StringDeserializer
            group.id: hangout
#    - Kafka:
#        codec: json
#        encoding: UTF8 # defaut UTF8
#        topic:
#          test-reporter: 1
#        consumer_settings:
#          group.id: hangout
#          zookeeper.connect: zookeeper:2181
#          auto.commit.interval.ms: "1000"
outputs:

#    - Elasticsearch:
#        cluster: elasticsearch
#        hosts:
#          - elasticsearch
#        index: 'hangout'
#        index_type: logs-${indexType} # default logs
#        document_id: ${id} # defautt null, generated by es
#        bulk_actions: 20000 #default 20000
#        bulk_size: 15 # default 15 MB
#        flush_interval: 10 # default 10 seconds
#        concurrent_requests: 0 # default 0, concurrent_requests设置成大于0的数, 意思着多线程处理, 以我应用的经验,还有是一定OOM风险的,强烈建议设置为0
#        timezone: "Asia/Shanghai" # defaut UTC 时区. 只用于生成索引名字的字符串格式化
#        sniff: false #default true

    - ElasticsearchHTTP:
        cluster: elasticsearch
        hosts:
          - elasticsearch
        index: 'metrics-%{+YYYY.MM.dd}'
        index_type: ${indexType}
        bulk_actions: 5 #default 20000
        sniff: false #default true